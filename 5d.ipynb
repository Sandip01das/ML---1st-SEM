{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "6e230223-2d69-4b97-b6ee-c1c605d4645e",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndata = pd.read_csv(\"play_tennis.csv\")\ndata = data.drop('day',axis=1)\nX = data.drop(columns=['play'])\ny = data['play']\ndata['play'] = data['play'].map({'No': 0, 'Yes': 1})\n\ndata.head(15)\n\ndef entropy(y):\n    value_counts = y.value_counts()\n    probabilities = value_counts / len(y)\n    entropy_value = -np.sum(probabilities * np.log2(probabilities.replace(0, 1)))\n    return entropy_value\nentropy(y)\n\ndef information_gain(y, feature):\n\n    total_entropy = entropy(y)\n    \n    unique_values = feature.unique()\n    weighted_entropies = 0\n\n    for value in unique_values:\n        subset_y = y[feature == value]\n        weighted_entropies += (len(subset_y) / len(y)) * entropy(subset_y)\n\n    return total_entropy - weighted_entropies\n\nfor column in data.columns:\n    if column != 'play':\n        feature = data[column]\n        ig = information_gain(y, feature)\n        print(f\"Feature: {column}, Information Gain: {ig:.4f}\")\n\nclass Node:\n    def __init__(self, feature=None, value=None, entropy=None, information_gain=None, left=None, right=None):\n        self.feature = feature\n        self.value = value\n        self.entropy = entropy\n        self.information_gain = information_gain\n        self.left = left\n        self.right = right\n\ndef build_decision_tree(X, y):\n    if entropy(y) == 0:\n        # If all instances have the same class, create a leaf node\n        return Node(value=y.iloc[0])\n\n    if X.empty:\n        # If no features left, create a leaf node with the majority class\n        return Node(value=y.value_counts().idxmax())\n\n    # Find the best feature to split on\n    best_feature = None\n    max_info_gain = 0\n\n    for feature_name in X.columns:\n        current_info_gain = information_gain(y, X[feature_name])\n        if current_info_gain > max_info_gain:\n            max_info_gain = current_info_gain\n            best_feature = feature_name\n\n    # Create a node with the best feature\n    node = Node(feature=best_feature, entropy=entropy(y), information_gain=max_info_gain, value={})\n\n    # Recursively build the left and right subtrees\n    unique_values = X[best_feature].unique()\n    for value in unique_values:\n        subset_X = X[X[best_feature] == value].drop(columns=[best_feature])\n        subset_y = y[X[best_feature] == value]\n        child_node = build_decision_tree(subset_X, subset_y)\n\n        if node.value is None:\n            node.value = {value: child_node}\n        else:\n            node.value[value] = child_node\n\n    return node\n\ndecision_tree = build_decision_tree(X, y)\n\n\ndef predict(node, instance):\n    if node.feature is None:\n        return node.value\n    else:\n        value = instance[node.feature]\n        if value in node.value:\n            return predict(node.value[value], instance)\n        else:\n            return node.value\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndecision_tree = build_decision_tree(X_train, y_train)\ny_pred = [predict(decision_tree, instance) for _, instance in X_test.iterrows()]\n\naccuracy_score = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy_score:.2f}\")\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint (tp,fp,fn,tn)\nacc=(tp+tn)/(tp+tn+fp+fn)\nprint (acc)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Feature: outlook, Information Gain: 0.2467\nFeature: temp, Information Gain: 0.0292\nFeature: humidity, Information Gain: 0.1518\nFeature: wind, Information Gain: 0.0481\nAccuracy: 1.00\n1 0 0 2\n1.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "2dbf55ec-82d4-41b7-99c2-d138d0aa21e4",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}