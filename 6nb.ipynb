{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c7e767ac-f979-46c2-b019-f1b4ce2f96f9",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nclass NaiveBayesClassifier:\n    def __init__(self):\n        self.class_probs = {}\n        self.feature_probs = {}\n\n    def fit(self, X, y):\n        num_samples, num_features = X.shape\n        unique_classes = np.unique(y)\n\n        for c in unique_classes:\n            # Calculate class probabilities\n            self.class_probs[c] = np.sum(y == c) / num_samples\n\n            # Calculate feature probabilities for each class\n            features_given_class = X[y == c]\n            self.feature_probs[c] = np.sum(features_given_class, axis=0) / np.sum(y == c)\n\n    def predict(self, X):\n        predictions = []\n\n        for sample in X:\n            class_scores = {}\n\n            for c, class_prob in self.class_probs.items():\n                # Calculate the probability of the sample belonging to each class\n                feature_probs_given_class = self.feature_probs[c]\n                log_prob = np.sum(np.log(sample * feature_probs_given_class + (1 - sample) * (1 - feature_probs_given_class)))\n                class_scores[c] = np.log(class_prob) + log_prob\n\n            # Predict the class with the highest probability\n            predicted_class = max(class_scores, key=class_scores.get)\n            predictions.append(predicted_class)\n\n        return predictions\n\ndata = pd.read_csv(\"play_tennis.csv\")\n\n# Convert categorical features to numerical values using label encoding\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']).columns:\n    data[column] = label_encoder.fit_transform(data[column])\n\nX = data.drop('play', axis=1).values\ny = data['play'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nnb_classifier = NaiveBayesClassifier()\nnb_classifier.fit(X_train, y_train)\npredictions = nb_classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Predictions:\", predictions)\nprint(\"Accuracy:\", accuracy)\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,predictions)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Predictions: [1, 0, 0]\nAccuracy: 0.6666666666666666\n1\n0\n1\n1\n0.6666666666666666\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "4ccc71cd-920b-47b5-ae0e-83fb5047c9c7",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}